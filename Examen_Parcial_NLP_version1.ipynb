{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Z5LHKSKq_x",
        "outputId": "8c551697-acf1-4118-e5e0-e501d5da097b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['all', 'models', 'are', 'wrong', 'a', 'model', 'is', 'wrong', 'some', 'useful', 'models', 'are', 'wrong']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Vocabulario\n",
        "vocabulario = {'<s>', '</s>', 'a', 'all', 'model', 'models', 'some', 'useful', 'wrong'}\n",
        "\n",
        "#corpus\n",
        "frase_1 =  \"all models are wrong\"\n",
        "frase_2 =  \"a model is wrong\"\n",
        "frase_3 =  \"some useful models are wrong\"\n",
        "lista_frases = [[frase_1], [frase_2], [frase_3]]\n",
        "\n",
        "def construir_corpus(lista_frases: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Constructs a corpus as a flat list of tokens from a list of sentences.\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "    for frase in lista_frases:\n",
        "        sentence = frase[0]\n",
        "        tokens = word_tokenize(sentence)\n",
        "        corpus.extend(tokens) # Extend corpus with tokens of this sentence\n",
        "    return corpus\n",
        "\n",
        "corpus = construir_corpus(lista_frases)\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloNGrama:\n",
        "    def __init__(self, n:int):\n",
        "        self.n = n\n",
        "        self.cuenta_ngrama = collections.Counter()\n",
        "        self.cuenta_contexto = collections.Counter()\n",
        "        self.vocabulario = vocabulario = {'<s>', '</s>', 'a', 'all', 'model', 'models', 'some', 'useful', 'wrong'}\n",
        "        self.total_ngrama = 0\n",
        "\n",
        "    def entrenar(self, corpus: List[str]):\n",
        "        for i in range(len(corpus) - self.n + 1):\n",
        "          if corpus[i] not in self.vocabulario:\n",
        "                self.vocabulario.add(corpus[i])\n",
        "          ngrama = tuple(corpus[i:i+self.n])\n",
        "          contexto = tuple(corpus[i:i+self.n-1])\n",
        "          self.cuenta_ngrama[ngrama] += 1\n",
        "          self.cuenta_contexto[contexto] += 1\n",
        "          self.total_ngrama += 1\n",
        "\n",
        "    def obtener_prob_ngram(self, ngrama: Tuple[str, ...]) -> float:\n",
        "        cuenta = self.cuenta_ngrama.get(ngrama, 0)\n",
        "        contexto = ngrama[:-1]\n",
        "        cuenta_contexto = self.cuenta_contexto.get(contexto, 0)\n",
        "        if cuenta_contexto == 0:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return cuenta / cuenta_contexto"
      ],
      "metadata": {
        "id": "JikAkeOpPZPj"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_bigrama = ModeloNGrama(n=2)\n",
        "modelo_bigrama.entrenar(corpus=corpus)\n",
        "#Probabilidades de todos los bigramas:\n",
        "print(f'probabilidad del bigrama a model {modelo_bigrama.obtener_prob_ngram((\"a\", \"model\"))}')\n",
        "print(f'probabilidad del bigrama model is {modelo_bigrama.obtener_prob_ngram((\"model\", \"is\"))}')\n",
        "print(f'probabilidad del bigrama is wrong {modelo_bigrama.obtener_prob_ngram((\"is\", \"wrong\"))}')\n",
        "print(f'probabilidad del bigrama are wrong {modelo_bigrama.obtener_prob_ngram((\"are\",\"wrong\"))}')\n",
        "\n",
        "print(f'probabilidad del bigrama inexistente a models {modelo_bigrama.obtener_prob_ngram((\"a\", \"models\"))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7H2wdzNQbiw",
        "outputId": "ef242578-ec04-4330-a7ad-8ceab9689979"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probabilidad del bigrama a model 1.0\n",
            "probabilidad del bigrama model is 1.0\n",
            "probabilidad del bigrama is wrong 1.0\n",
            "probabilidad del bigrama are wrong 1.0\n",
            "probabilidad del bigrama inexistente a models 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Suavizado add-k para bigrama\n",
        "def add_k_smoothing_bigram(corpus, k):\n",
        "    # Conteo de bigramas y unigrams\n",
        "    bigram_counts = {}\n",
        "    unigram_counts = {}\n",
        "\n",
        "    # Construir bigramas\n",
        "    for i in range(len(corpus) - 1):\n",
        "        bigram = (corpus[i], corpus[i + 1])\n",
        "        unigram = corpus[i]\n",
        "\n",
        "        # Conteo de bigramas\n",
        "        if bigram in bigram_counts:\n",
        "            bigram_counts[bigram] += 1\n",
        "        else:\n",
        "            bigram_counts[bigram] = 1\n",
        "\n",
        "        # Conteo de unigrams\n",
        "        if unigram in unigram_counts:\n",
        "            unigram_counts[unigram] += 1\n",
        "        else:\n",
        "            unigram_counts[unigram] = 1\n",
        "\n",
        "    # Contar el último unigramo\n",
        "    last_word = corpus[-1]\n",
        "    if last_word in unigram_counts:\n",
        "        unigram_counts[last_word] += 1\n",
        "    else:\n",
        "        unigram_counts[last_word] = 1\n",
        "\n",
        "    # Tamaño del vocabulario\n",
        "    V = len(unigram_counts)\n",
        "\n",
        "    # Cálculo de las probabilidades suavizadas para bigramas\n",
        "    add_k_probabilities = {}\n",
        "    for bigram, bigram_count in bigram_counts.items():\n",
        "        w_n_1 = bigram[0]  # w_{n-1}\n",
        "        # Aplicando la ecuación P_Add-k(w_n | w_{n-1}) = (C(w_{n-1}w_n) + k) / (C(w_{n-1}) + kV)\n",
        "        add_k_probabilities[bigram] = (bigram_count + k) / (unigram_counts[w_n_1] + k * V)\n",
        "\n",
        "    # Probabilidad para un bigrama no visto\n",
        "    add_k_probabilities[('<a>', '<models>')] = k / (V * (V + k))\n",
        "\n",
        "    return add_k_probabilities"
      ],
      "metadata": {
        "id": "P2wliQkYsu3y"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 0.1\n",
        "add_k_prob_bigrams = add_k_smoothing_bigram(corpus, k)\n",
        "print(\"\\nProbabilidades de bigramas suavizadas con Add-k (k=0.1):\")\n",
        "for bigram, prob in add_k_prob_bigrams.items():\n",
        "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqe4LqC0tyL3",
        "outputId": "660ce404-171b-4403-dc7e-74c5d6259df1"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Probabilidades de bigramas suavizadas con Add-k (k=0.1):\n",
            "P(models | all) = 0.5789\n",
            "P(are | models) = 0.7241\n",
            "P(wrong | are) = 0.7241\n",
            "P(a | wrong) = 0.2821\n",
            "P(model | a) = 0.5789\n",
            "P(is | model) = 0.5789\n",
            "P(wrong | is) = 0.5789\n",
            "P(some | wrong) = 0.2821\n",
            "P(useful | some) = 0.5789\n",
            "P(models | useful) = 0.5789\n",
            "P(<models> | <a>) = 0.0012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def backoff_unigram(unigram_counts, total_tokens):\n",
        "    # Calcula las probabilidades de unigramas\n",
        "    unigram_probs = {}\n",
        "    for word, count in unigram_counts.items():\n",
        "        unigram_probs[word] = count / total_tokens\n",
        "    return unigram_probs\n",
        "\n",
        "def backoff_bigram(bigram_counts, unigram_counts, total_tokens):\n",
        "    # Calcula las probabilidades de bigramas con retroceso a unigramas\n",
        "    bigram_probs = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if unigram_counts[w1] > 0:\n",
        "            bigram_probs[(w1, w2)] = count / unigram_counts[w1]\n",
        "        else:\n",
        "            bigram_probs[(w1, w2)] = backoff_unigram(unigram_counts, total_tokens).get(w2, 1 / total_tokens)\n",
        "    return bigram_probs\n",
        "\n",
        "def ngram_backoff(unigram_probs, bigram_probs, alpha):\n",
        "    def backoff_prob(w_n, w_n1):\n",
        "        # Bigram\n",
        "        if (w_n1, w_n) in bigram_probs:\n",
        "            return alpha * bigram_probs[(w_n1, w_n)]\n",
        "        # Unigram\n",
        "        else:\n",
        "            return alpha * unigram_probs.get(w_n, 1 / len(unigram_probs))\n",
        "    return backoff_prob\n",
        "\n"
      ],
      "metadata": {
        "id": "YVcYwpTkUbk7"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"probabilidades con backoff\")\n",
        "alpha = 0.4\n",
        "backoff_prob = ngram_backoff(backoff_unigram, backoff_bigram, alpha)\n",
        "#Probabilidades de todos los bigramas:\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Rd-A7jP9WuYJ",
        "outputId": "d8ebdc19-a71b-44e5-dd21-b322f7e405ed"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probabilidades con backoff\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-a55da0d3efd7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbackoff_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackoff_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackoff_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Probabilidades de todos los bigramas:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'probabilidad del bigrama a model {backoff_bigram(\"a\", \"model\", corpus)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'probabilidad del bigrama model is {backoff_bigram(\"model\", \"is\", 0.4)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'probabilidad del bigrama is wrong {backoff_bigram(\"is\", \"wrong\", 0.4)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-145-3802f4220861>\u001b[0m in \u001b[0;36mbackoff_bigram\u001b[0;34m(bigram_counts, unigram_counts, total_tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Calcula las probabilidades de bigramas con retroceso a unigramas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbigram_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigram_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munigram_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mbigram_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munigram_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ejercicio 2"
      ],
      "metadata": {
        "id": "bqy2VORpYMIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def leer_archivo(ruta):\n",
        "    with open(ruta, 'r', encoding='utf-8') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "ruta = 'infopankki.en-es.es'\n",
        "archivo = leer_archivo(ruta)\n",
        "\n",
        "def tokenizar_linea(linea):\n",
        "    palabras = []\n",
        "    for palabra in linea.split():\n",
        "      #eliminacion de Stopwords\n",
        "      if palabra.endswith(','):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('.'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('!'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('?'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith(':'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith(';'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('('):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith(')'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('['):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith(']'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('{'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('}'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('\"'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith(\"'\"):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('`'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('`'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('`'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('`'):\n",
        "        palabra = palabra[:-1]\n",
        "      if palabra.endswith('`'):\n",
        "        palabra = palabra[:-1]\n",
        "      if len(palabra) not in ['a', 'al', 'de', 'o', 'e' 'del']:\n",
        "          palabras.append(palabra)\n",
        "    return palabras\n",
        "\n",
        "def lematizar_tokens(tokens):\n",
        "  patron_amar = re.compile(r'amar[a-z]*')\n",
        "  patron_estudiar = re.compile(r'estudi[a-z]*')\n",
        "  for token in tokens:\n",
        "    if re.find(patron_amar, token): #implementar una expresion regular mas precisa\n",
        "      token = 'amar'\n",
        "    if re.find(patron_estudiar, token): #implementar una expresion regular mas precisa\n",
        "      token = 'estudiar'\n",
        "\n",
        "def stemizar_tokens(tokens):\n",
        "  patron_terminacion_s = re.compile(r'[a-z]*s')\n",
        "  patron_terminacion_es = re.compile(r'[a-z]*es')\n",
        "  for token in tokens:\n",
        "    if re.find(patron_terminacion_s, token): #implementar una expresion regular mas precisa\n",
        "      token = token[:-1]\n",
        "    if re.find(patron_terminacion_es, token): #implementar una expresion regular mas precisa\n",
        "      token = token[:-2]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "#Preprosesamiento de texto\n",
        "def preprocesar_archivo(archivo, num_muestras=10000):\n",
        "    vocabulario_inicial = []\n",
        "    num_lineas = 0\n",
        "    for linea in archivo:\n",
        "        tokens = tokenizar_linea(linea)\n",
        "        tokens = lematizar_tokens(tokens)\n",
        "        tokens = stemizar_tokens(tokens)\n",
        "        for token in tokens:\n",
        "          vocabulario_inicial.append(token)\n",
        "        num_lineas += 1\n",
        "        if num_lineas >= num_muestras:\n",
        "            break\n",
        "    for palabra in vocabulario:\n",
        "      apariciones = vocabulario.count(palabra)\n",
        "      if apariciones < 5:\n",
        "        vocabulario.remove(palabra)\n",
        "    return vocabulario\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "1DJfx0gZZCnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CP_iQGBUj8Pr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
